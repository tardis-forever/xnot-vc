{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Required libraries are listed in `requirements.txt`.\n",
    "\n",
    "Run `%pip install -r requirements.txt` to install them.\n",
    "\n",
    "We conducted experiments on GPU (`g.1.8` - 8-core CPU + 1xV100 GPU configuration in [Yandex Datasphere](https://datasphere.yandex.ru/)) and assume that GPU will be used in any replication."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "import torch, torchaudio\n",
    "from xnot_matcher import XNOTNeighborsVC\n",
    "from xnot import XNot\n",
    "\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_curve\n",
    "from jiwer import cer, wer\n",
    "\n",
    "device = 'cuda'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For intelligibility evaluation and some experiments later in this work we will be using commercially available ARS and TTS service [Yandex SpeechKit](https://cloud.yandex.ru/en/services/speechkit)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "from speechkit import configure_credentials, creds\n",
    "from speechkit import model_repository\n",
    "from speechkit.stt import AudioProcessingType\n",
    "\n",
    "configure_credentials(\n",
    "    yandex_credentials=creds.YandexCredentials(\n",
    "        api_key=os.environ['api_key'],\n",
    "    )\n",
    ")\n",
    "\n",
    "model = model_repository.recognition_model()\n",
    "\n",
    "model.model = 'general:rc'\n",
    "model.language = 'en-US'\n",
    "model.audio_processing_type = AudioProcessingType.Full"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For speaker identity conservation evaluation we will be using pretrained model for x-vector retrieval from [SpeechBrain](https://huggingface.co/speechbrain/spkrec-xvect-voxceleb)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779140b602c545aea46c9bb592d1031e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hyperparams.yaml:   0%|          | 0.00/2.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ad66efdf8e48c6972e40a8d1b147dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding_model.ckpt:   0%|          | 0.00/16.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511e9fdd3c1345f6aebd0d0d6afd159a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mean_var_norm_emb.ckpt:   0%|          | 0.00/3.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef275a36f0e4de2b22c6485721915f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "classifier.ckpt:   0%|          | 0.00/15.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031ec7972d624098af218b30eaef84a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "label_encoder.txt:   0%|          | 0.00/129k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-xvect-voxceleb\", savedir=\"pretrained_models/spkrec-xvect-voxceleb\", run_opts={\"device\": device})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a baseline for comparison, we will use the best checkpoint provided by original authors of the [paper](https://arxiv.org/abs/2305.18975) - [kNN-VC with prematched HiFiGAN](https://github.com/bshall/knn-vc/releases/download/v0.1/prematch_g_02500000.pt).\n",
    "\n",
    "`XNOTNeighborsVC` is our modification of `KNeighborsVC` that allows for `xnot` usage during inference."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/bshall/knn-vc/zipball/master\" to /tmp/xdg_cache/torch/hub/master.zip\n",
      "Downloading: \"https://github.com/bshall/knn-vc/releases/download/v0.1/prematch_g_02500000.pt\" to /tmp/xdg_cache/torch/hub/checkpoints/prematch_g_02500000.pt\n",
      "100%|██████████| 63.1M/63.1M [00:00<00:00, 118MB/s] \n",
      "Downloading: \"https://github.com/bshall/knn-vc/releases/download/v0.1/WavLM-Large.pt\" to /tmp/xdg_cache/torch/hub/checkpoints/WavLM-Large.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n",
      "[HiFiGAN] Generator loaded with 16,523,393 parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.18G/1.18G [00:09<00:00, 138MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WavLM-Large loaded with 315,453,120 parameters.\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "knn_vc = torch.hub.load('bshall/knn-vc', 'knn_vc', prematched=True, trust_repo=True, pretrained=True, device=device)\n",
    "xnot_vc = XNOTNeighborsVC(knn_vc.wavlm, knn_vc.hifigan, knn_vc.h, device=device).eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Following the original [paper](https://arxiv.org/abs/2305.18975), we are going to be using LibriSpeech test-clean split from http://www.openslr.org/12. Download this split and place unpacked folder in the root of this repository.\n",
    "\n",
    "For convenience, we are going to use preprocessed textual ground truth references for this split from [LibriSpeech Alignments](https://github.com/CorentinJ/librispeech-alignments). Download the texts in condensed format from the `README.md` and also unpack it in the root of the repository."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "libri_folder = 'data/LibriSpeech/test-clean/'\n",
    "speakers = list(sorted(os.listdir(libri_folder)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "all_speaker_audios = {}\n",
    "for speaker in speakers:\n",
    "    files = glob.glob(f'{libri_folder}/{speaker}/*/*.flac', recursive=True)\n",
    "    all_speaker_audios[speaker] = files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Following the original [paper](https://arxiv.org/abs/2305.18975) once again, we choose 5 sample audios at random for each speaker."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "chosen = {}\n",
    "for speaker in speakers:\n",
    "    files = glob.glob(f'{libri_folder}/{speaker}/*/*.flac', recursive=True)\n",
    "    chosen[speaker] = np.random.choice(files, 5, replace=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For convenience, we precalculate feature embeddings for chosen audios:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "matching_sets = {}\n",
    "\n",
    "for speaker in tqdm(speakers):\n",
    "    audios = []\n",
    "    for filename in chosen[speaker]:\n",
    "        audio, _ = torchaudio.load(filename)\n",
    "        audios.append(audio)\n",
    "    matching_sets[speaker] = knn_vc.get_matching_set(audios).cpu()\n",
    "\n",
    "torch.save(matching_sets, 'data/matchings/matching_sets')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As the number of pairs is approximately square of the number of speakers (thus, growing fast), we consider only the first 10 speakers (out of 40) in our research due to time and budget constraints.\n",
    "\n",
    "---\n",
    "\n",
    "### Basic setup\n",
    "\n",
    "Each audio is used as a single source and converted to all the other considered speakers via both original `knn` algorithm and our `xnot` modification of it. For `xnot`, during **all** the experiments we are going to use `[1, 2, 4]` as values for `w` hyperparameter of `XNot`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "n_speakers = 10\n",
    "speakers_list = speakers[:n_speakers]\n",
    "\n",
    "\n",
    "for src_speaker in tqdm(speakers_list):\n",
    "    for target_speaker in speakers_list:\n",
    "        if src_speaker == target_speaker:\n",
    "            continue\n",
    "        print(f'Conversing {src_speaker=} to {target_speaker=}')\n",
    "        for filename in chosen[src_speaker]:\n",
    "            audio, _ = torchaudio.load(filename)\n",
    "            query_seq = knn_vc.get_features(audio)\n",
    "            idx = filename.split('/')[-1].split('.')[0]\n",
    "\n",
    "            for i, W in enumerate([1.0, 2.0, 4.0]):\n",
    "                path = f'w-{int(W)}/target-{target_speaker}-idx-{idx}-src-{src_speaker}'\n",
    "                if os.path.exists(f'data/x_nots/{path}'):\n",
    "                    continue\n",
    "                out_wav_xnot, xnot = xnot_vc.match(query_seq, matching_sets[target_speaker], topk=4, algorithm='xnot', W=W, max_steps=200)\n",
    "                torchaudio.save(f'data/audios/xnot/{path}.wav', out_wav_xnot[None].cpu(), 16000)\n",
    "                torch.save(\n",
    "                {\n",
    "                    'state_dict': xnot.state_dict()\n",
    "                }, f'data/x_nots/{path}')\n",
    "            path = f'target-{target_speaker}-idx-{idx}-src-{src_speaker}'\n",
    "            out_wav_knn, _ = xnot_vc.match(query_seq, matching_sets[target_speaker], topk=4, algorithm='knn')\n",
    "            torchaudio.save(f'data/audios/knn/{path}.wav', out_wav_knn[None].cpu(), 16000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### V2 experiment setup\n",
    "\n",
    "For each source-target speaker pair we use **all 5** available audios for each speaker to train a single `XNot` checkpoint, which we use to convert all 5 audios afterwards."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "%%time\n",
    "\n",
    "for src_speaker in tqdm(speakers_list):\n",
    "    for target_speaker in speakers_list:\n",
    "        if src_speaker == target_speaker:\n",
    "            continue\n",
    "        print(f'Conversing {src_speaker=} to {target_speaker=}')\n",
    "        src_audios = []\n",
    "\n",
    "        for filename in chosen[src_speaker]:\n",
    "            audio, _ = torchaudio.load(filename)\n",
    "            src_audios.append(audio)\n",
    "\n",
    "        query_matchings = knn_vc.get_matching_set(src_audios)\n",
    "        for i, W in enumerate([1.0, 2.0, 4.0]):\n",
    "\n",
    "            x_not = XNot(query_matchings.shape[1], device)\n",
    "            x_not = x_not.train(True)\n",
    "            x_not.fit(query_matchings.to(device), matching_sets[target_speaker].to(device), max_steps=200, t_iters=10, cost=\"sq_cost\", batch_size=64, W=W)\n",
    "\n",
    "            for filename in chosen[src_speaker]:\n",
    "                audio, _ = torchaudio.load(filename)\n",
    "                query_seq = knn_vc.get_features(audio)\n",
    "                idx = filename.split('/')[-1].split('.')[0]\n",
    "\n",
    "                path = f'w-{int(W)}/target-{target_speaker}-idx-{idx}-src-{src_speaker}'\n",
    "                if os.path.exists(f'data/2/x_nots/{path}'):\n",
    "                    continue\n",
    "                out_wav_xnot, _ = xnot_vc.match(query_seq, matching_sets[target_speaker], topk=4, algorithm='xnot', W=W, max_steps=200, x_not=x_not)\n",
    "                torchaudio.save(f'data/2/audios/xnot/{path}.wav', out_wav_xnot[None].cpu(), 16000)\n",
    "                torch.save(\n",
    "                {\n",
    "                    'state_dict': x_not.state_dict()\n",
    "                }, f'data/2/x_nots/{path}')\n",
    "                path = f'target-{target_speaker}-idx-{idx}-src-{src_speaker}'\n",
    "                out_wav_knn, _ = xnot_vc.match(query_seq, matching_sets[target_speaker], topk=4, algorithm='knn')\n",
    "                torchaudio.save(f'data/2/audios/knn/{path}.wav', out_wav_knn[None].cpu(), 16000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ablation setup\n",
    "\n",
    "For each source-target speaker pair we use only one pretrained (in the previous step) `XNot` checkpoint to convert all 4 of remaining audios."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "%%time\n",
    "n_speakers = 10\n",
    "speakers_list = speakers[:n_speakers]\n",
    "xnot = XNot(1024, device)  # 1024 is the original embedding size\n",
    "\n",
    "for src_speaker in tqdm(speakers_list):\n",
    "    for target_speaker in speakers_list:\n",
    "        if src_speaker == target_speaker:\n",
    "            continue\n",
    "        print(f'Conversing {src_speaker=} to {target_speaker=}')\n",
    "        existing_xnot_filename = chosen[src_speaker][0]\n",
    "        existing_xnot_idx = existing_xnot_filename.split('/')[-1].split('.')[0]\n",
    "\n",
    "        for filename in chosen[src_speaker][1:]:\n",
    "            audio, _ = torchaudio.load(filename)\n",
    "            query_seq = knn_vc.get_features(audio)\n",
    "            idx = filename.split('/')[-1].split('.')[0]\n",
    "\n",
    "            for i, W in enumerate([1.0, 2.0, 4.0]):\n",
    "                path = f'w-{int(W)}/target-{target_speaker}-existing-{existing_xnot_idx}-idx-{idx}-src-{src_speaker}'\n",
    "                xnot.load_state_dict(torch.load(f'data/x_nots/w-{int(W)}/target-{target_speaker}-idx-{existing_xnot_idx}-src-{src_speaker}')['state_dict'])\n",
    "                out_wav_xnot, _ = xnot_vc.match(query_seq, matching_sets[target_speaker], topk=4, algorithm='xnot', W=W, max_steps=200, x_not=xnot)\n",
    "                torchaudio.save(f'data/audios/reconstructed/{path}.wav', out_wav_xnot[None].cpu(), 16000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "\n",
    "### Speaker identity conservation\n",
    "\n",
    "We couldn't find the computation code for `EER` used by authors, and our version definitely differs from theirs (our is capped at 1.0 and not 0.5). Thus, we compute our version of this metric on results produced by original algorithm to produce comparable values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def compute_eer(label, pred, positive_label=1):\n",
    "    fpr, tpr, threshold = roc_curve(label, pred, pos_label=positive_label)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    return eer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`EER` is computated over x-vector cosine similarity scores."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "sim = torch.nn.CosineSimilarity().to(device)\n",
    "\n",
    "knn_similarities = []\n",
    "\n",
    "for target_speaker in tqdm(chosen, total=len(chosen)):\n",
    "    conversions = glob.glob(f'data/audios/knn/target-{target_speaker}*', recursive=True)\n",
    "    source_audios = np.random.choice(all_speaker_audios[target_speaker], size=len(conversions))\n",
    "\n",
    "    for source, conversion in zip(source_audios, conversions):\n",
    "        source_embedding = classifier.encode_batch(torchaudio.load(source)[0]).squeeze(1)\n",
    "        converted_embedding = classifier.encode_batch(torchaudio.load(conversion)[0]).squeeze(1)\n",
    "        knn_similarities.append(sim(source_embedding, converted_embedding).cpu().item())\n",
    "\n",
    "\n",
    "gt_similarities = []\n",
    "\n",
    "for target_speaker in np.random.choice(speakers, size=len(knn_similarities)):\n",
    "    src_speaker = target_speaker\n",
    "    while src_speaker == target_speaker:\n",
    "        src_speaker = np.random.choice(speakers)\n",
    "\n",
    "    target_audio = np.random.choice(all_speaker_audios[target_speaker], size=1)[0]\n",
    "    source_audio = np.random.choice(all_speaker_audios[src_speaker], size=1)[0]\n",
    "\n",
    "    source_embedding = classifier.encode_batch(torchaudio.load(source_audio)[0]).squeeze(1)\n",
    "    converted_embedding = classifier.encode_batch(torchaudio.load(target_audio)[0]).squeeze(1)\n",
    "    gt_similarities.append(sim(source_embedding, converted_embedding).cpu().item())\n",
    "\n",
    "\n",
    "preds_knn = np.array(knn_similarities + gt_similarities)\n",
    "targets = np.array([0. for _ in knn_similarities] + [1. for _ in gt_similarities])\n",
    "knn_eer = compute_eer(targets, preds_knn, positive_label=1)\n",
    "\n",
    "results = []\n",
    "for W in [1.0, 2.0, 4.0]:\n",
    "    similarities = []\n",
    "\n",
    "    for target_speaker in tqdm(chosen, total=len(chosen)):\n",
    "        conversions = glob.glob(f'data/audios/xnot/w-{int(W)}/target-{target_speaker}*', recursive=True)\n",
    "        source_audios = np.random.choice(all_speaker_audios[target_speaker], size=len(conversions))\n",
    "\n",
    "        for source, conversion in zip(source_audios, conversions):\n",
    "            source_embedding = classifier.encode_batch(torchaudio.load(source)[0]).squeeze(1)\n",
    "            converted_embedding = classifier.encode_batch(torchaudio.load(conversion)[0]).squeeze(1)\n",
    "            similarities.append(sim(source_embedding, converted_embedding).cpu().item())\n",
    "\n",
    "    preds_xnot = np.array(similarities + gt_similarities)\n",
    "    targets = np.array([0. for _ in similarities] + [1. for _ in gt_similarities])\n",
    "\n",
    "    results.append(compute_eer(targets, preds_xnot, positive_label=1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "knn_similarities_2 = []\n",
    "\n",
    "for target_speaker in tqdm(chosen, total=len(chosen)):\n",
    "    conversions = glob.glob(f'data/2/audios/knn/target-{target_speaker}*', recursive=True)\n",
    "    source_audios = np.random.choice(all_speaker_audios[target_speaker], size=len(conversions))\n",
    "\n",
    "    for source, conversion in zip(source_audios, conversions):\n",
    "        source_embedding = classifier.encode_batch(torchaudio.load(source)[0]).squeeze(1)\n",
    "        converted_embedding = classifier.encode_batch(torchaudio.load(conversion)[0]).squeeze(1)\n",
    "        knn_similarities_2.append(sim(source_embedding, converted_embedding).cpu().item())\n",
    "\n",
    "gt_similarities = []\n",
    "\n",
    "for target_speaker in np.random.choice(speakers, size=len(knn_similarities_2)):\n",
    "    src_speaker = target_speaker\n",
    "    while src_speaker == target_speaker:\n",
    "        src_speaker = np.random.choice(speakers)\n",
    "\n",
    "    target_audio = np.random.choice(all_speaker_audios[target_speaker], size=1)[0]\n",
    "    source_audio = np.random.choice(all_speaker_audios[src_speaker], size=1)[0]\n",
    "\n",
    "    source_embedding = classifier.encode_batch(torchaudio.load(source_audio)[0]).squeeze(1)\n",
    "    converted_embedding = classifier.encode_batch(torchaudio.load(target_audio)[0]).squeeze(1)\n",
    "    gt_similarities.append(sim(source_embedding, converted_embedding).cpu().item())\n",
    "\n",
    "\n",
    "preds_knn = np.array(knn_similarities_2 + gt_similarities)\n",
    "targets = np.array([0. for _ in knn_similarities_2] + [1. for _ in gt_similarities])\n",
    "knn_eer_2 = compute_eer(targets, preds_knn, positive_label=1)\n",
    "\n",
    "results_2 = []\n",
    "for W in [1.0, 2.0, 4.0]:\n",
    "    similarities = []\n",
    "\n",
    "    for target_speaker in tqdm(chosen, total=len(chosen)):\n",
    "        conversions = glob.glob(f'data/2/audios/xnot/w-{int(W)}/target-{target_speaker}*', recursive=True)\n",
    "        source_audios = np.random.choice(all_speaker_audios[target_speaker], size=len(conversions))\n",
    "\n",
    "        for source, conversion in zip(source_audios, conversions):\n",
    "            source_embedding = classifier.encode_batch(torchaudio.load(source)[0]).squeeze(1)\n",
    "            converted_embedding = classifier.encode_batch(torchaudio.load(conversion)[0]).squeeze(1)\n",
    "            similarities.append(sim(source_embedding, converted_embedding).cpu().item())\n",
    "\n",
    "    preds_xnot = np.array(similarities + gt_similarities)\n",
    "    targets = np.array([0. for _ in similarities] + [1. for _ in gt_similarities])\n",
    "\n",
    "    results_2.append(compute_eer(targets, preds_xnot, positive_label=1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "results_reconstructed = []\n",
    "for W in [1.0, 2.0, 4.0]:\n",
    "    similarities = []\n",
    "\n",
    "    for target_speaker in tqdm(chosen, total=len(chosen)):\n",
    "        conversions = glob.glob(f'data/audios/reconstructed/w-{int(W)}/target-{target_speaker}*', recursive=True)\n",
    "        source_audios = np.random.choice(all_speaker_audios[target_speaker], size=len(conversions))\n",
    "\n",
    "        for source, conversion in zip(source_audios, conversions):\n",
    "            source_embedding = classifier.encode_batch(torchaudio.load(source)[0]).squeeze(1)\n",
    "            converted_embedding = classifier.encode_batch(torchaudio.load(conversion)[0]).squeeze(1)\n",
    "            similarities.append(sim(source_embedding, converted_embedding).cpu().item())\n",
    "\n",
    "    preds_xnot = np.array(similarities + gt_similarities)\n",
    "    targets = np.array([0. for _ in similarities] + [1. for _ in gt_similarities])\n",
    "\n",
    "    results_reconstructed.append(compute_eer(targets, preds_xnot, positive_label=1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Intelligibility\n",
    "\n",
    "We obtain recognitions of generated audio from SpeechKit:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "recognitions = defaultdict(dict)\n",
    "\n",
    "for speaker, filenames in tqdm(chosen.items(), total=len(chosen)):\n",
    "    for filename in filenames:\n",
    "        if filename in recognitions['src']:\n",
    "            continue\n",
    "        recognition = model.transcribe_file(filename)\n",
    "        assert len(recognition) == 1\n",
    "        recognitions['src'][filename] = recognition[0].raw_text\n",
    "\n",
    "with open('recognitions-src.json', 'w') as f:\n",
    "    json.dump(recognitions['src'], f, ensure_ascii=False, indent=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "knn_wavs = glob.glob(f'data/audios/knn/*.wav', recursive=True)\n",
    "xnot_all_wavs = glob.glob(f'data/audios/xnot/*/*.wav', recursive=True)\n",
    "xnot_reconstructed_wavs = glob.glob(f'data/audios/reconstructed/*/*.wav', recursive=True)\n",
    "\n",
    "knn_2_wavs = glob.glob(f'data/2/audios/knn/*.wav', recursive=True)\n",
    "xnot_2_all_wavs = glob.glob(f'data/2/audios/xnot/*/*.wav', recursive=True)\n",
    "\n",
    "for filename in tqdm(knn_wavs):\n",
    "    if filename in recognitions['knn']:\n",
    "        continue\n",
    "    recognition = model.transcribe_file(filename)\n",
    "    assert len(recognition) == 1\n",
    "    recognitions['knn'][filename] = recognition[0].raw_text\n",
    "\n",
    "for filename in tqdm(xnot_all_wavs):\n",
    "    if filename in recognitions['xnot']:\n",
    "        continue\n",
    "    recognition = model.transcribe_file(filename)\n",
    "    assert len(recognition) == 1\n",
    "    recognitions['xnot'][filename] = recognition[0].raw_text\n",
    "\n",
    "\n",
    "for filename in tqdm(xnot_reconstructed_wavs):\n",
    "    if filename in recognitions['reconstructed']:\n",
    "        continue\n",
    "    recognition = model.transcribe_file(filename)\n",
    "    assert len(recognition) == 1\n",
    "    recognitions['reconstructed'][filename] = recognition[0].raw_text\n",
    "\n",
    "\n",
    "for filename in tqdm(knn_2_wavs):\n",
    "    if filename in recognitions['2-knn']:\n",
    "        continue\n",
    "\n",
    "    recognition = model.transcribe_file(filename)\n",
    "    assert len(recognition) == 1\n",
    "    recognitions['2-knn'][filename] = recognition[0].raw_text\n",
    "    print(recognitions['2-knn'][filename])\n",
    "\n",
    "for filename in tqdm(xnot_2_all_wavs):\n",
    "    if filename in recognitions['2-xnot']:\n",
    "        continue\n",
    "    recognition = model.transcribe_file(filename)\n",
    "    assert len(recognition) == 1\n",
    "    recognitions['2-xnot'][filename] = recognition[0].raw_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "with open('recognitions.json', 'w') as f:\n",
    "    json.dump(recognitions, f, ensure_ascii=False, indent=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ground truth transcripts retrieval:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "text_files = glob.glob(f'{libri_folder}/**/*trans.txt', recursive=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "gt_texts = {}\n",
    "\n",
    "for file in text_files:\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            idx, text = line.split(maxsplit=1)\n",
    "            gt_texts[idx] = text.lower()\n",
    "\n",
    "with open('gt_texts.json', 'w') as f:\n",
    "    json.dump(gt_texts, f, ensure_ascii=False, indent=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`WER` and `CER` calculation:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "wer_results = {}\n",
    "\n",
    "knn_wers = []\n",
    "knn_cers = []\n",
    "\n",
    "for filename, rec_text in recognitions['knn'].items():\n",
    "    idx = filename.split('-', maxsplit=3)[-1].rsplit('-', maxsplit=2)[0]\n",
    "    knn_wers.append(wer(gt_texts[idx], rec_text))\n",
    "    knn_cers.append(cer(gt_texts[idx], rec_text))\n",
    "\n",
    "for W in [1.0, 2.0, 4.0]:\n",
    "\n",
    "    xnot_wavs = glob.glob(f'data/audios/xnot/w-{int(W)}/*.wav', recursive=True)\n",
    "\n",
    "    xnot_wers = []\n",
    "    xnot_cers = []\n",
    "\n",
    "    for filename, rec_text in recognitions['xnot'].items():\n",
    "        idx = filename.split('-', maxsplit=4)[-1].rsplit('-', maxsplit=2)[0]\n",
    "        xnot_wers.append(wer(gt_texts[idx], rec_text))\n",
    "        xnot_cers.append(cer(gt_texts[idx], rec_text))\n",
    "\n",
    "    wer_results[int(W)] = (xnot_wers, xnot_cers)\n",
    "\n",
    "\n",
    "wer_results_reconstructed = {}\n",
    "\n",
    "for W in [1.0, 2.0, 4.0]:\n",
    "\n",
    "    xnot_wavs = glob.glob(f'data/audios/reconstructed/w-{int(W)}/*.wav', recursive=True)\n",
    "\n",
    "    xnot_wers = []\n",
    "    xnot_cers = []\n",
    "\n",
    "    for filename, rec_text in recognitions['reconstructed'].items():\n",
    "        idx = filename.split('-', maxsplit=8)[-1].rsplit('-', maxsplit=2)[0]\n",
    "        xnot_wers.append(wer(gt_texts[idx], rec_text))\n",
    "        xnot_cers.append(cer(gt_texts[idx], rec_text))\n",
    "\n",
    "    wer_results_reconstructed[int(W)] = (xnot_wers, xnot_cers)\n",
    "\n",
    "\n",
    "\n",
    "wer_results_2 = {}\n",
    "\n",
    "knn_wers_2 = []\n",
    "knn_cers_2 = []\n",
    "\n",
    "for filename, rec_text in recognitions['2-knn'].items():\n",
    "    if 'target-ru' not in filename:\n",
    "        idx = filename.split('-', maxsplit=3)[-1].rsplit('-', maxsplit=2)[0]\n",
    "    else:\n",
    "        idx = filename.split('-', maxsplit=4)[-1].rsplit('-', maxsplit=2)[0]\n",
    "\n",
    "    knn_wers_2.append(wer(gt_texts[idx], rec_text))\n",
    "    knn_cers_2.append(cer(gt_texts[idx], rec_text))\n",
    "\n",
    "for W in [1.0, 2.0, 4.0]:\n",
    "    xnot_wavs = glob.glob(f'data/2/audios/xnot/w-{int(W)}/*.wav', recursive=True)\n",
    "\n",
    "    xnot_wers = []\n",
    "    xnot_cers = []\n",
    "\n",
    "    for filename, rec_text in recognitions['2-xnot'].items():\n",
    "        if 'target-ru' not in filename:\n",
    "            idx = filename.split('-', maxsplit=4)[-1].rsplit('-', maxsplit=2)[0]\n",
    "        else:\n",
    "            idx = filename.split('-', maxsplit=5)[-1].rsplit('-', maxsplit=2)[0]\n",
    "        xnot_wers.append(wer(gt_texts[idx], rec_text))\n",
    "        xnot_cers.append(cer(gt_texts[idx], rec_text))\n",
    "\n",
    "    wer_results_2[int(W)] = (xnot_wers, xnot_cers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a precaution, we also calculate `WER` and `CER` of chosen ASR service on original audios:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "gt_wers = []\n",
    "gt_cers = []\n",
    "\n",
    "for filename, rec_text in recognitions['src'].items():\n",
    "    idx = filename.split('/')[-1].split('.')[0]\n",
    "    gt_wers.append(wer(gt_texts[idx], rec_text))\n",
    "    gt_cers.append(cer(gt_texts[idx], rec_text))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cross-lingual conversion\n",
    "\n",
    "We synthesized 5 randomly chosen texts from `War and Peace` by Leo Tolstoy (in Russian) via [Yandex SpeechKit](https://cloud.yandex.ru/en/services/speechkit) TTS. Then, we translated it into considered English speakers and vise-versa."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "tts_model = model_repository.synthesis_model()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import string"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "ru_speaker = 'ru-tts'\n",
    "\n",
    "with open('data/ru_texts.txt') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        text = line.strip()\n",
    "        assert i not in gt_texts\n",
    "        gt_texts[str(i)] = text.lower().translate(str.maketrans('', '', string.punctuation))  # rem-punkt normalization\n",
    "        tts_model.synthesize(text).export(f'data/LibriSpeech/test-clean/{ru_speaker}/{i}.wav', format='wav')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "%%time\n",
    "chosen[ru_speaker] = glob.glob(f'{libri_folder}/{ru_speaker}/*.wav', recursive=True)\n",
    "\n",
    "for filename in chosen[ru_speaker]:\n",
    "    audios = []\n",
    "    audio, _ = torchaudio.load(filename)\n",
    "    audios.append(audio)\n",
    "    matching_sets[ru_speaker] = knn_vc.get_matching_set(audios).cpu()\n",
    "\n",
    "torch.save(matching_sets, 'data/matchings/matching_sets-ru')\n",
    "\n",
    "\n",
    "for src_speaker in tqdm(speakers_list):\n",
    "    target_speaker = ru_speaker\n",
    "    print(f'Conversing {src_speaker=} to {target_speaker=}')\n",
    "    for filename in chosen[src_speaker]:\n",
    "        audio, _ = torchaudio.load(filename)\n",
    "        query_seq = knn_vc.get_features(audio)\n",
    "        idx = filename.split('/')[-1].split('.')[0]\n",
    "\n",
    "        for i, W in enumerate([1.0, 2.0, 4.0]):\n",
    "            path = f'w-{int(W)}/target-{target_speaker}-idx-{idx}-src-{src_speaker}'\n",
    "            if os.path.exists(f'data/ru/x_nots/{path}'):\n",
    "                continue\n",
    "            out_wav_xnot, xnot = xnot_vc.match(query_seq, matching_sets[target_speaker], topk=4, algorithm='xnot', W=W, max_steps=200)\n",
    "            torchaudio.save(f'data/ru/audios/xnot/{path}.wav', out_wav_xnot[None].cpu(), 16000)\n",
    "            torch.save(\n",
    "            {\n",
    "                'state_dict': xnot.state_dict()\n",
    "            }, f'data/ru/x_nots/{path}')\n",
    "        path = f'target-{target_speaker}-idx-{idx}-src-{src_speaker}'\n",
    "        out_wav_knn, _ = xnot_vc.match(query_seq, matching_sets[target_speaker], topk=4, algorithm='knn')\n",
    "        torchaudio.save(f'data/ru/audios/knn/{path}.wav', out_wav_knn[None].cpu(), 16000)\n",
    "\n",
    "\n",
    "\n",
    "for target_speaker in tqdm(speakers_list):\n",
    "    src_speaker = ru_speaker\n",
    "    print(f'Conversing {src_speaker=} to {target_speaker=}')\n",
    "    for filename in chosen[src_speaker]:\n",
    "        audio, _ = torchaudio.load(filename)\n",
    "        query_seq = knn_vc.get_features(audio)\n",
    "        idx = filename.split('/')[-1].split('.')[0]\n",
    "\n",
    "        for i, W in enumerate([1.0, 2.0, 4.0]):\n",
    "            path = f'w-{int(W)}/target-{target_speaker}-idx-{idx}-src-{src_speaker}'\n",
    "            if os.path.exists(f'data/ru/x_nots/{path}'):\n",
    "                continue\n",
    "            out_wav_xnot, xnot = xnot_vc.match(query_seq, matching_sets[target_speaker], topk=4, algorithm='xnot', W=W, max_steps=200)\n",
    "            torchaudio.save(f'data/ru/audios/xnot/{path}.wav', out_wav_xnot[None].cpu(), 16000)\n",
    "            torch.save(\n",
    "            {\n",
    "                'state_dict': xnot.state_dict()\n",
    "            }, f'data/ru/x_nots/{path}')\n",
    "        path = f'target-{target_speaker}-idx-{idx}-src-{src_speaker}'\n",
    "        out_wav_knn, _ = xnot_vc.match(query_seq, matching_sets[target_speaker], topk=4, algorithm='knn')\n",
    "        torchaudio.save(f'data/ru/audios/knn/{path}.wav', out_wav_knn[None].cpu(), 16000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "ru_model = model_repository.recognition_model()\n",
    "\n",
    "ru_model.model = 'general:rc'\n",
    "ru_model.language = 'ru-RU'\n",
    "ru_model.audio_processing_type = AudioProcessingType.Full"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "knn_ru_wavs = glob.glob(f'data/ru/audios/knn/*.wav', recursive=True)\n",
    "xnot_ru_all_wavs = glob.glob(f'data/ru/audios/xnot/*/*.wav', recursive=True)\n",
    "\n",
    "for filename in tqdm(knn_ru_wavs):\n",
    "    if filename in recognitions['ru-knn']:\n",
    "        continue\n",
    "    if f'target-{ru_speaker}' in filename:\n",
    "        m = model\n",
    "    else:\n",
    "        m = ru_model\n",
    "    recognition = m.transcribe_file(filename)\n",
    "    assert len(recognition) == 1\n",
    "    recognitions['ru-knn'][filename] = recognition[0].raw_text\n",
    "\n",
    "for filename in tqdm(xnot_ru_all_wavs):\n",
    "    if filename in recognitions['ru-xnot']:\n",
    "        continue\n",
    "    if f'target-{ru_speaker}' in filename:\n",
    "        m = model\n",
    "    else:\n",
    "        m = ru_model\n",
    "    recognition = m.transcribe_file(filename)\n",
    "    assert len(recognition) == 1\n",
    "    recognitions['ru-xnot'][filename] = recognition[0].raw_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "ru_wer_results = {}\n",
    "\n",
    "ru_knn_wers = []\n",
    "ru_knn_cers = []\n",
    "\n",
    "for filename, rec_text in recognitions['ru-knn'].items():\n",
    "    if 'target-ru' not in filename:\n",
    "        idx = filename.split('-', maxsplit=3)[-1].rsplit('-', maxsplit=3)[0]\n",
    "    else:\n",
    "\n",
    "        idx = filename.split('-', maxsplit=4)[-1].rsplit('-', maxsplit=2)[0]\n",
    "\n",
    "    ru_knn_wers.append(wer(gt_texts[idx], rec_text))\n",
    "    ru_knn_cers.append(cer(gt_texts[idx], rec_text))\n",
    "\n",
    "for W in [1.0, 2.0, 4.0]:\n",
    "\n",
    "    ru_xnot_wavs = glob.glob(f'data/ru/audios/xnot/w-{int(W)}/*.wav', recursive=True)\n",
    "\n",
    "    ru_xnot_wers = []\n",
    "    ru_xnot_cers = []\n",
    "\n",
    "    for filename, rec_text in recognitions['ru-xnot'].items():\n",
    "        if 'target-ru' not in filename:\n",
    "            idx = filename.split('-', maxsplit=4)[-1].rsplit('-', maxsplit=3)[0]\n",
    "        else:\n",
    "\n",
    "            idx = filename.split('-', maxsplit=5)[-1].rsplit('-', maxsplit=2)[0]\n",
    "        ru_xnot_wers.append(wer(gt_texts[idx], rec_text))\n",
    "        ru_xnot_cers.append(cer(gt_texts[idx], rec_text))\n",
    "\n",
    "    ru_wer_results[int(W)] = (ru_xnot_wers, ru_xnot_cers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "ru_knn_similarities = []\n",
    "\n",
    "for target_speaker in tqdm(list(chosen.keys()) + [ru_speaker], total=len(chosen)):\n",
    "    conversions = glob.glob(f'data/ru/audios/knn/target-{target_speaker}*', recursive=True)\n",
    "    source_audios = np.random.choice(all_speaker_audios[target_speaker], size=len(conversions))\n",
    "\n",
    "    for source, conversion in zip(source_audios, conversions):\n",
    "        source_embedding = classifier.encode_batch(torchaudio.load(source)[0]).squeeze(1)\n",
    "        converted_embedding = classifier.encode_batch(torchaudio.load(conversion)[0]).squeeze(1)\n",
    "        ru_knn_similarities.append(sim(source_embedding, converted_embedding).cpu().item())\n",
    "\n",
    "\n",
    "gt_similarities = []\n",
    "\n",
    "for target_speaker in np.random.choice(speakers + [ru_speaker], size=len(ru_knn_similarities)):\n",
    "    src_speaker = target_speaker\n",
    "    while src_speaker == target_speaker:\n",
    "        src_speaker = np.random.choice(speakers + [ru_speaker])\n",
    "\n",
    "    target_audio = np.random.choice(all_speaker_audios[target_speaker], size=1)[0]\n",
    "    source_audio = np.random.choice(all_speaker_audios[src_speaker], size=1)[0]\n",
    "\n",
    "    source_embedding = classifier.encode_batch(torchaudio.load(source_audio)[0]).squeeze(1)\n",
    "    converted_embedding = classifier.encode_batch(torchaudio.load(target_audio)[0]).squeeze(1)\n",
    "    gt_similarities.append(sim(source_embedding, converted_embedding).cpu().item())\n",
    "\n",
    "\n",
    "preds_knn = np.array(ru_knn_similarities + gt_similarities)\n",
    "targets = np.array([0. for _ in ru_knn_similarities] + [1. for _ in gt_similarities])\n",
    "knn_eer_ru = compute_eer(targets, preds_knn, positive_label=1)\n",
    "\n",
    "results_ru = []\n",
    "for W in [1.0, 2.0, 4.0]:\n",
    "    similarities = []\n",
    "\n",
    "    for target_speaker in tqdm(list(chosen.keys()) + [ru_speaker], total=len(chosen)):\n",
    "        conversions = glob.glob(f'data/ru/audios/xnot/w-{int(W)}/target-{target_speaker}*', recursive=True)\n",
    "        source_audios = np.random.choice(all_speaker_audios[target_speaker], size=len(conversions))\n",
    "\n",
    "        for source, conversion in zip(source_audios, conversions):\n",
    "            source_embedding = classifier.encode_batch(torchaudio.load(source)[0]).squeeze(1)\n",
    "            converted_embedding = classifier.encode_batch(torchaudio.load(conversion)[0]).squeeze(1)\n",
    "            similarities.append(sim(source_embedding, converted_embedding).cpu().item())\n",
    "\n",
    "    preds_xnot = np.array(similarities + gt_similarities)\n",
    "    targets = np.array([0. for _ in similarities] + [1. for _ in gt_similarities])\n",
    "\n",
    "    results_ru.append(compute_eer(targets, preds_xnot, positive_label=1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "exp_results = dict(\n",
    "    knn_eer=knn_eer,\n",
    "    results=results,\n",
    "    knn_eer_2=knn_eer_2,\n",
    "    results_2=results_2,\n",
    "    results_reconstructed=results_reconstructed,\n",
    "    knn_eer_ru=knn_eer_ru,\n",
    "    results_ru=results_ru,\n",
    "    knn_wers=knn_wers,\n",
    "    knn_cers=knn_cers,\n",
    "    wer_results=wer_results,\n",
    "    wer_results_reconstructed=wer_results_reconstructed,\n",
    "    wer_results_2=wer_results_2,\n",
    "    knn_wers_2=knn_wers_2,\n",
    "    knn_cers_2=knn_cers_2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(exp_results, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "2167db63-a270-4122-a084-b1124f1aeeee",
  "notebookPath": "xnot_voice_conversion/xnot_demo-gpu-Copy2.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
