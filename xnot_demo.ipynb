{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellId": "4s7a5bupgyx3tdtnb6ilg7"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "import torch, torchaudio\n",
    "from xnot_matcher import XNOTNeighborsVC\n",
    "\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_curve\n",
    "from jiwer import cer, wer\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellId": "jvxz5bjv7q3zonqccrddk"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "from speechkit import configure_credentials, creds\n",
    "from speechkit import model_repository\n",
    "from speechkit.stt import AudioProcessingType\n",
    "\n",
    "configure_credentials(\n",
    "    yandex_credentials=creds.YandexCredentials(\n",
    "        api_key=os.environ['api_key'],\n",
    "    )\n",
    ")\n",
    "\n",
    "model = model_repository.recognition_model()\n",
    "\n",
    "model.model = 'general:rc'\n",
    "model.language = 'en-US'\n",
    "model.audio_processing_type = AudioProcessingType.Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellId": "nkken0amgb3868yoqjvbh"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779140b602c545aea46c9bb592d1031e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hyperparams.yaml:   0%|          | 0.00/2.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ad66efdf8e48c6972e40a8d1b147dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding_model.ckpt:   0%|          | 0.00/16.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511e9fdd3c1345f6aebd0d0d6afd159a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mean_var_norm_emb.ckpt:   0%|          | 0.00/3.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef275a36f0e4de2b22c6485721915f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "classifier.ckpt:   0%|          | 0.00/15.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031ec7972d624098af218b30eaef84a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "label_encoder.txt:   0%|          | 0.00/129k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-xvect-voxceleb\", savedir=\"pretrained_models/spkrec-xvect-voxceleb\", run_opts={\"device\": device})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellId": "n4q9innp3kqvvomm7cnquo",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L8If-TOe4BZO",
    "outputId": "0ed08fba-ad75-48a9-be34-b4976f286bb7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/bshall/knn-vc/zipball/master\" to /tmp/xdg_cache/torch/hub/master.zip\n",
      "Downloading: \"https://github.com/bshall/knn-vc/releases/download/v0.1/prematch_g_02500000.pt\" to /tmp/xdg_cache/torch/hub/checkpoints/prematch_g_02500000.pt\n",
      "100%|██████████| 63.1M/63.1M [00:00<00:00, 118MB/s] \n",
      "Downloading: \"https://github.com/bshall/knn-vc/releases/download/v0.1/WavLM-Large.pt\" to /tmp/xdg_cache/torch/hub/checkpoints/WavLM-Large.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n",
      "[HiFiGAN] Generator loaded with 16,523,393 parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.18G/1.18G [00:09<00:00, 138MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WavLM-Large loaded with 315,453,120 parameters.\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "knn_vc = torch.hub.load('bshall/knn-vc', 'knn_vc', prematched=True, trust_repo=True, pretrained=True, device=device)\n",
    "xnot_vc = XNOTNeighborsVC(knn_vc.wavlm, knn_vc.hifigan, knn_vc.h, device=device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellId": "4hte62ivtce3fgcihrvruf"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "libri_folder = 'data/LibriSpeech/test-clean/'\n",
    "speakers = list(sorted(os.listdir(libri_folder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellId": "soqpi0y659isfii5o12pdj"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "all_speaker_audios = {}\n",
    "for speaker in speakers:\n",
    "    files = glob.glob(f'{libri_folder}/{speaker}/*/*.flac', recursive=True)\n",
    "    all_speaker_audios[speaker] = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellId": "at0j73d995kawa70sud67v"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "chosen = {}\n",
    "for speaker in speakers:\n",
    "    files = glob.glob(f'{libri_folder}/{speaker}/*/*.flac', recursive=True)\n",
    "    chosen[speaker] = np.random.choice(files, 5, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellId": "sep6n7mvnp1u78kr8xcwi"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "matching_sets = {}\n",
    "\n",
    "for speaker in tqdm(speakers):\n",
    "    audios = []\n",
    "    for filename in chosen[speaker]:\n",
    "        audio, _ = torchaudio.load(filename)\n",
    "        audios.append(audio)\n",
    "    matching_sets[speaker] = knn_vc.get_matching_set(audios).cpu()\n",
    "    \n",
    "torch.save(matching_sets, 'data/matchings/matching_sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "0b61244k0mrj89wgf44cxm",
    "execution_id": "44e3d53c-9046-4736-bcea-f8ea6d43e29a",
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "n_speakers = 10\n",
    "speakers_list = speakers[:n_speakers]\n",
    "\n",
    "\n",
    "for src_speaker in tqdm(speakers_list):\n",
    "    for target_speaker in speakers_list:\n",
    "        if src_speaker == target_speaker:\n",
    "            continue\n",
    "        print(f'Conversing {src_speaker=} to {target_speaker=}')\n",
    "        for filename in chosen[src_speaker]:\n",
    "            audio, _ = torchaudio.load(filename)\n",
    "            query_seq = knn_vc.get_features(audio)\n",
    "            idx = filename.split('/')[-1].split('.')[0]\n",
    "\n",
    "            for i, W in enumerate([1.0, 2.0, 4.0]):\n",
    "                path = f'w-{int(W)}/target-{target_speaker}-idx-{idx}-src-{src_speaker}'\n",
    "                if os.path.exists(f'data/x_nots/{path}'):\n",
    "                    continue\n",
    "                out_wav_xnot, xnot = xnot_vc.match(query_seq, matching_sets[target_speaker], topk=4, algorithm='xnot', W=W, max_steps=200)\n",
    "                torchaudio.save(f'data/audios/xnot/{path}.wav', out_wav_xnot[None].cpu(), 16000)\n",
    "                torch.save(\n",
    "                {\n",
    "                    'state_dict': xnot.state_dict()\n",
    "                }, f'data/x_nots/{path}')\n",
    "            path = f'target-{target_speaker}-idx-{idx}-src-{src_speaker}'\n",
    "            out_wav_knn, _ = xnot_vc.match(query_seq, matching_sets[target_speaker], topk=4, algorithm='knn')\n",
    "            torchaudio.save(f'data/audios/knn/{path}.wav', out_wav_knn[None].cpu(), 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "36yqxq089hc5fsfg5pxq6k",
    "execution_id": "d96b78d7-ad68-4bc8-8084-97b8bbf8623d"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def compute_eer(label, pred, positive_label=1):\n",
    "    fpr, tpr, threshold = roc_curve(label, pred, pos_label=positive_label)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    return eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "imbavx0o0cup7qxrxyhzp",
    "execution_id": "18cdb6ed-f049-40b4-a637-3857b3cf72d4"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "sim = torch.nn.CosineSimilarity().to(device)\n",
    "\n",
    "knn_similarities = []\n",
    "\n",
    "for target_speaker in tqdm(chosen, total=len(chosen)):\n",
    "    conversions = glob.glob(f'data/audios/knn/target-{target_speaker}*', recursive=True)\n",
    "    source_audios = np.random.choice(all_speaker_audios[target_speaker], size=len(conversions))\n",
    "\n",
    "    for source, conversion in zip(source_audios, conversions):\n",
    "        source_embedding = classifier.encode_batch(torchaudio.load(source)[0]).squeeze(1)\n",
    "        converted_embedding = classifier.encode_batch(torchaudio.load(conversion)[0]).squeeze(1)\n",
    "        knn_similarities.append(sim(source_embedding, converted_embedding).cpu().item())\n",
    "\n",
    "        \n",
    "gt_similarities = []\n",
    "\n",
    "for target_speaker in np.random.choice(speakers, size=len(knn_similarities)):\n",
    "    src_speaker = target_speaker\n",
    "    while src_speaker == target_speaker:\n",
    "        src_speaker = np.random.choice(speakers)\n",
    "\n",
    "    target_audio = np.random.choice(all_speaker_audios[target_speaker], size=1)\n",
    "    source_audio = np.random.choice(all_speaker_audios[src_speaker], size=1)\n",
    "\n",
    "    source_embedding = classifier.encode_batch(torchaudio.load(source_audio)[0]).squeeze(1)\n",
    "    converted_embedding = classifier.encode_batch(torchaudio.load(target_audio)[0]).squeeze(1)\n",
    "    gt_similarities.append(sim(source_embedding, converted_embedding).cpu().item())\n",
    "\n",
    "        \n",
    "preds_knn = np.array(knn_similarities + gt_similarities)\n",
    "targets = np.array([1. for _ in knn_similarities] + [0. for _ in gt_similarities])\n",
    "knn_eer = compute_eer(targets, preds_knn, positive_label=0)\n",
    "\n",
    "results = []\n",
    "for W in [1.0, 2.0, 4.0]:\n",
    "    similarities = []\n",
    "\n",
    "    for target_speaker in tqdm(chosen, total=len(chosen)):\n",
    "        conversions = glob.glob(f'data/audios/xnot/w-{int(W)}/target-{target_speaker}*', recursive=True)\n",
    "        source_audios = np.random.choice(all_speaker_audios[target_speaker], size=len(conversions))\n",
    "\n",
    "        for source, conversion in zip(source_audios, conversions):\n",
    "            source_embedding = classifier.encode_batch(torchaudio.load(source)[0]).squeeze(1)\n",
    "            converted_embedding = classifier.encode_batch(torchaudio.load(conversion)[0]).squeeze(1)\n",
    "            similarities.append(sim(source_embedding, converted_embedding).cpu().item())\n",
    "    \n",
    "    preds_xnot = np.array(similarities + gt_similarities)\n",
    "    targets = np.array([1. for _ in similarities] + [0. for _ in gt_similarities])\n",
    "\n",
    "    results.append(compute_eer(targets, preds_xnot, positive_label=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "afpzdbf0utb20gzhka8lwa",
    "execution_id": "c8d91e33-e7ab-41a5-993f-ca5682211c97"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "recognitions = defaultdict(dict)\n",
    "\n",
    "for speaker, filenames in tqdm(chosen.items(), total=len(chosen)):\n",
    "    for filename in filenames:\n",
    "        if filename in recognitions['src']:\n",
    "            continue\n",
    "        recognition = model.transcribe_file(filename)\n",
    "        assert len(recognition) == 1\n",
    "        recognitions['src'][filename] = recognition[0].raw_text\n",
    "\n",
    "with open('recognitions-src.json', 'w') as f:\n",
    "    json.dump(recognitions['src'], f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ua22wm70j44pxv2ydbrk",
    "execution_id": "b3ed7aaa-177f-4603-a577-be7c675ba0bc"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "knn_wavs = glob.glob(f'data/audios/knn/*.wav', recursive=True)\n",
    "xnot_all_wavs = glob.glob(f'data/audios/xnot/*/*.wav', recursive=True)\n",
    "\n",
    "for filename in tqdm(knn_wavs):\n",
    "    if filename in recognitions['knn']:\n",
    "        continue\n",
    "    recognition = model.transcribe_file(filename)\n",
    "    assert len(recognition) == 1\n",
    "    recognitions['knn'][filename] = recognition[0].raw_text\n",
    "    \n",
    "for filename in tqdm(xnot_all_wavs):\n",
    "    if filename in recognitions['xnot']:\n",
    "        continue\n",
    "    recognition = model.transcribe_file(filename)\n",
    "    assert len(recognition) == 1\n",
    "    recognitions['xnot'][filename] = recognition[0].raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ss25sdyh52owtk6etrfto",
    "execution_id": "03e2f1aa-754a-483f-b7fe-8d0a0926327d"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "with open('recognitions.json', 'w') as f:\n",
    "    json.dump(recognitions, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "izgso7noxhsjlkvuoqo2p",
    "execution_id": "5c1cac56-3d94-4148-852a-8dc7297a626c"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "text_files = glob.glob(f'{libri_folder}/**/*trans.txt', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "mw9cmjtbpmyq3adjba9t",
    "execution_id": "e4bfbef3-4393-4984-bb5d-d7cc42d936e1"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "gt_texts = {}\n",
    "\n",
    "for file in text_files:\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            idx, text = line.split(maxsplit=1)\n",
    "            gt_texts[idx] = text.lower()\n",
    "            \n",
    "with open('gt_texts.json', 'w') as f:\n",
    "    json.dump(gt_texts, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "u22r2w6yh3pdm01dij5rtm",
    "execution_id": "c0b77022-1a0b-43e8-9d8b-ad7c3adf8e9c"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "wer_results = {}\n",
    "\n",
    "knn_wers = []\n",
    "knn_cers = []\n",
    "\n",
    "for filename, rec_text in recognitions['knn'].items():\n",
    "    idx = filename.split('-', maxsplit=3)[-1].rsplit('-', maxsplit=2)[0]\n",
    "    knn_wers.append(wer(gt_texts[idx], rec_text))\n",
    "    knn_cers.append(wer(gt_texts[idx], rec_text))\n",
    "\n",
    "for W in [1.0, 2.0, 4.0]:\n",
    "\n",
    "    xnot_wavs = glob.glob(f'data/audios/xnot/w-{int(W)}/*.wav', recursive=True)\n",
    "\n",
    "    xnot_wers = []\n",
    "    xnot_cers = []\n",
    "\n",
    "    for filename, rec_text in recognitions['xnot'].items():\n",
    "        idx = filename.split('-', maxsplit=4)[-1].rsplit('-', maxsplit=2)[0]\n",
    "        xnot_wers.append(wer(gt_texts[idx], rec_text))\n",
    "        \n",
    "    wer_results[int(W)] = (xnot_wers, xnot_cers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "2g0ef4w6isixzcr98oy7n9",
    "execution_id": "592e8a82-c9fb-4a44-9d8b-7b9c9a951dd3"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "gt_wers = []\n",
    "gt_cers = []\n",
    "\n",
    "for filename, rec_text in recognitions['src'].items():\n",
    "    idx = filename.split('/')[-1].split('.')[0]\n",
    "    gt_wers.append(wer(gt_texts[idx], rec_text))\n",
    "    gt_cers.append(cer(gt_texts[idx], rec_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "0ahsz8hzx41vwifqk09g7jg",
    "execution_id": "f65ef180-3d13-468a-bbc0-47501493d676"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "2167db63-a270-4122-a084-b1124f1aeeee",
  "notebookPath": "xnot_voice_conversion/xnot_demo-gpu-Copy2.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
